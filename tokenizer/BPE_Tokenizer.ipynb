{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "- Tokenization is the process of breaking down a piece of text into smaller, manageable chunks called \"tokens\". There are several tokenization methods:\n",
        "\n",
        "Text : \"I am Nishchal\"\n",
        "\n",
        "- Character Tokenization - [\"I\", \" \", \"a\", \"m\", \" \", \"N\", \"i\", \"s\", \"h\", \"c\", \"h\", \"a\", \"l\"]\n",
        "- Word Tokenization - [\"I\", \"am\", \"Nishchal\"]\n",
        "- Sentence Tokenization - [\"I am Nishchal\"]\n",
        "- Subword Tokenization - [\"I\", \"am\", \" \", \"Nish\", \"ch\", \"al\"]\n",
        "\n",
        "Subword Tokenization like BPE are the backbone of GPT and BERT Models."
      ],
      "metadata": {
        "id": "HU5Yyu5gryUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte Pair Encoding\n",
        "\n",
        "- The Goal of the BPE algorithm is to build a vocabulary of commonly occuring subwords or even complete words to group meaningful group of characters together."
      ],
      "metadata": {
        "id": "JS1AQsq9diXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE Algorithm Outline\n",
        "\n",
        "1. Identify Frequency Pairs\n",
        "- In each iterations, scan the text to find the most commonly occuring pair of bytes.\n",
        "\n",
        "2. Replace or Record\n",
        "- Replace the pair with a new placeholder ID (i.e new token ids which are currently not in use).\n",
        "- Record the mapping in the lookup table.\n",
        "- The size of the lookup table is hyperparameter (i.e Vocabulary size).\n",
        "\n",
        "3. Repeat until No Gain\n",
        "- Keep repeating step 1 and 2, continually merging the most frequent pairs.\n",
        "- stop when no pair occurs more than the threshold number.\n",
        "\n",
        "4. Decoding\n",
        "- To restore the original text, reverse the process by substituting each ID with it's corresponding pair, using the lookup table."
      ],
      "metadata": {
        "id": "n-KijFjVd7Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT4 Tokenizer\n",
        "- The key difference in the GPT4 and basic tokenizer is the use of regex pattern to split the input text.\n",
        "- Resulting in creation of more effective and meaningful tokens"
      ],
      "metadata": {
        "id": "lmilxLBV200a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regex Patterns for Tokenization\n",
        "\n",
        "The text is split using the following regex pattern:\n",
        "\n",
        "```\n",
        "r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "```\n",
        "\n",
        "- (?i:'s|'t|'re|'ve|'m|'ll|'d) : This extracts the contraction in English. \\\n",
        "For Example -> I'm, It's, he'd, can't, you're, they've\n",
        "\n",
        "- [^\\r\\n\\p{L}\\p{N}]?\\p{L}+ : This extracts the sequence of any characters, letters, and numerical digits except \"\\r\" and \"\\n\".\n",
        "\n",
        "- \\p{N}{1,3} : This extracts numbers upto 3 digits as tokens.\n",
        "\n",
        "- ?[^\\s\\p{L}\\p{N}]+[\\r\\n]* : This extracts the special characters, punctuations, or symbols possibly preceded by space and follwed by optional newlines.\n",
        "\n",
        "- \\s*[\\r\\n]+ : This extracts newlines(\\n) or carriage returns(\\r) proceded by whitespace.\n",
        "\n",
        "- \\s+(?!\\S) : This extracts the trailing whitespace at the end of a line or before a newline. Also this doesnâ€™t extract anu non-whitespsce characters.\n",
        "\n",
        "- \\s+ : This extracts any remaining sequence of whitespaces that might not be captured by the previous parts."
      ],
      "metadata": {
        "id": "ZU2N_BCr3Vbf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JGWQx5V0dbVl"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "import torch.nn as nn\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders\n",
        "\n",
        "class BPETokenizer(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, special_tokens=None):\n",
        "    super().__init__()\n",
        "    self.pattern = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "    self.vocab_size = vocab_size\n",
        "    self.merges = {}\n",
        "    self.vocab = {}\n",
        "    self.special_tokens = special_tokens or [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<user>\", \"<assistant>\"]\n",
        "    self.special_token_ids = {}\n",
        "\n",
        "  # Find Consecutive Pairs\n",
        "  def get_stats(self, token_ids, stats=None):\n",
        "    \"\"\"Count frequency of consecutive pairs\"\"\"\n",
        "\n",
        "    if stats is None:\n",
        "      stats = {}\n",
        "    for pair in zip(token_ids, token_ids[1:]):\n",
        "      stats[pair] = stats.get(pair, 0) + 1\n",
        "\n",
        "    return stats\n",
        "\n",
        "  # Merge Token Ids\n",
        "  def merge(self, token_ids, pair, new_idx):\n",
        "    \"\"\"Merge all occurrences of pair into new_idx\"\"\"\n",
        "\n",
        "    new_token_ids = []\n",
        "    i = 0\n",
        "    while i < len(token_ids):\n",
        "      if ((i + 1 < len(token_ids))  and (token_ids[i] == pair[0]) and (token_ids[i + 1] == pair[1])):\n",
        "        new_token_ids.append(new_idx)\n",
        "        i += 2\n",
        "      else:\n",
        "        new_token_ids.append(token_ids[i])\n",
        "        i += 1\n",
        "\n",
        "    return new_token_ids\n",
        "\n",
        "  def train(self, clean_text, verbose=False):\n",
        "    \"\"\"Train BPE Tokenizer on clean text\"\"\"\n",
        "\n",
        "    num_special_tokens = len(self.special_tokens)\n",
        "    effective_vocab_size = self.vocab_size - num_special_tokens\n",
        "    num_merges = effective_vocab_size - 256\n",
        "\n",
        "    text_chunks = re.findall(self.pattern, clean_text)\n",
        "    token_ids = [list(chunk.encode('utf-8')) for chunk in text_chunks]\n",
        "\n",
        "    self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "\n",
        "    for i in range(num_merges):\n",
        "      stats = {}\n",
        "\n",
        "      for chunk_token in token_ids:\n",
        "        self.get_stats(chunk_token, stats)\n",
        "\n",
        "      if not stats:\n",
        "        break\n",
        "\n",
        "      top_pair = max(stats, key=stats.get)\n",
        "      new_token_id = 256 + i\n",
        "\n",
        "      if verbose:\n",
        "        print(f'merged : {top_pair} -> {new_token_id }')\n",
        "\n",
        "      token_ids = [self.merge(chunk_token, top_pair, new_token_id) for chunk_token in token_ids]\n",
        "      self.merges[top_pair] = new_token_id\n",
        "      self.vocab[new_token_id] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
        "\n",
        "    self.add_special_tokens_to_vocab()\n",
        "\n",
        "  # Adding Special Tokens to Vocabulary\n",
        "  def add_special_tokens_to_vocab(self):\n",
        "\n",
        "    if not self.special_tokens:\n",
        "      return\n",
        "\n",
        "    start_id = max(self.vocab.keys()) + 1\n",
        "    for i, token in enumerate(self.special_tokens):\n",
        "      token_id = start_id + i\n",
        "      self.vocab[token_id] = token.encode('utf-8')\n",
        "      self.special_token_ids[token] = token_id\n",
        "\n",
        "    print(f\"Special tokens: {self.special_token_ids}\")\n",
        "\n",
        "  def save_hf(self, path):\n",
        "    \"\"\"Convert to HuggingFace tokenizer and save\"\"\"\n",
        "\n",
        "    hf_vocab = {}\n",
        "    for token_id, token_bytes in self.vocab.items():\n",
        "      try:\n",
        "        token_str = token_bytes.decode('utf-8')\n",
        "      except UnicodeDecodeError:\n",
        "        token_str = ''.join(f'\\\\x{b:02x}' for b in token_bytes)\n",
        "      hf_vocab[token_str] = token_id\n",
        "\n",
        "    hf_merges = []\n",
        "    for (token1_id, token2_id), merged_id in self.merges.items():\n",
        "      try:\n",
        "        token1_str = self.vocab[token1_id].decode('utf-8')\n",
        "        token2_str = self.vocab[token2_id].decode('utf-8')\n",
        "      except UnicodeDecodeError:\n",
        "        token1_str = ''.join(f'\\\\x{b:02x}' for b in self.vocab[token1_id])\n",
        "        token2_str = ''.join(f'\\\\x{b:02x}' for b in self.vocab[token2_id])\n",
        "\n",
        "      hf_merges.append((token1_str, token2_str))\n",
        "\n",
        "    tokenizer = Tokenizer(models.BPE(\n",
        "        vocab=hf_vocab,\n",
        "        merges=hf_merges,\n",
        "        unk_token=\"<unk>\" if \"<unk>\" in self.special_tokens else None\n",
        "    ))\n",
        "\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "        pre_tokenizers.Split(\n",
        "            pattern=self.pattern,\n",
        "            behavior=\"isolated\"\n",
        "        )\n",
        "    ])\n",
        "    tokenizer.decoder = decoders.Sequence([\n",
        "        decoders.BPEDecoder()\n",
        "    ])\n",
        "\n",
        "    special_tokens_list = []\n",
        "    for token in self.special_tokens:\n",
        "        special_tokens_list.append(token)\n",
        "\n",
        "    if special_tokens_list:\n",
        "        tokenizer.add_special_tokens(special_tokens_list)\n",
        "\n",
        "    tokenizer.save(path)\n",
        "    print(f\"HF tokenizer saved to {path}\")\n",
        "    print(f\"Contains: {len(hf_vocab)} vocab, {len(hf_merges)} merges, {len(self.special_tokens)} special tokens\")\n",
        "\n",
        "    return tokenizer"
      ]
    }
  ]
}